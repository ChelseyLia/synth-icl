wandb: WARNING Config item 'seed' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'model' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'data' was locked by 'sweep' (ignored update).
  0%|          | 0/100001 [00:00<?, ?it/s]
CONFIG
├── seed
│   └── 9656
├── seed_eval
│   └── 2000
├── seed_test
│   └── 3000
├── device
│   └── cuda
├── train
│   └── do: true
│       do_save: false
│       reset_every: null
│       dtype: float32
│       compile: false
│       do_amp: false
│       batch_size: 128
│       grad_clip: 1.0
│       iters: 100001
│       samples: null
│       log_every: 1000
│       save_every: 100000
│       save_path: ./out
│       num_workers: 0
│       do_early_stop: false
│       early_stop_patience: 5
│       early_stop_metric: loss
│       early_stop_tol: 0.001
│       early_stop_start_iter: 20000.2
│       early_stop_acc: null
│       parallel_loss: true
│       merge_embeds: false
│       merge_type: sum
│
├── eval
│   └── do: true
│       split: both
│       batch_size: 1
│       every: 5000
│       every_samples: null
│       iters: 1000
│
├── scheduler
│   └── decay_lr: true
│       warmup_iters: 20000.2
│       lr_decay_iters: 100001
│       min_lr: 0.0001
│
├── optimizer
│   └── lr: 0.001
│       _name_: adamw
│       weight_decay: 0
│       betas:
│       - 0.9
│       - 0.95
│
├── log_level
│   └── info
├── examples_to_log
│   └── 3
├── log_batch_idx
│   └── [0, 1]
├── wandb
│   └── project: icl-arch
│
├── save_checkpoints
│   └── False
├── nl_icl
│   └── do: false
│       checkpoint_path: null
│       hf_path: null
│       task: sentiment
│       n_seeds: 10
│       min_examples_per_class: 0
│       max_examples_per_class: 9
│       do_full_vocab: true
│
├── do_count_param_only
│   └── False
├── model
│   └── _name_: mamba
│       d_model: 64
│       n_layer: 12
│       norm_epsilon: 1.0e-05
│       rms_norm: false
│       fused_add_norm: false
│       residual_in_fp32: false
│       max_seq_len: 4096
│
└── data
    └── _name_: linear-regression
        curriculum:
          dims:
            start: 5
            end: 5
            inc: 1
            interval: 2000
          points_train:
            start: 32
            end: 32
            inc: 2
            interval: 2000
          points_val:
            start: 1024
            end: 1024
            inc: 2
            interval: 2000
        task: linear_regression
        data: gaussian
        task_kwargs: {}
        n_dims: 5
        train_noise: 0
        val_noise: 0
mamba.backbone.layers.0.mixer.A_log: 2048
mamba.backbone.layers.0.mixer.D: 128
mamba.backbone.layers.0.mixer.in_proj.weight: 16384
mamba.backbone.layers.0.mixer.conv1d.weight: 512
mamba.backbone.layers.0.mixer.conv1d.bias: 128
mamba.backbone.layers.0.mixer.x_proj.weight: 4608
mamba.backbone.layers.0.mixer.dt_proj.weight: 512
mamba.backbone.layers.0.mixer.dt_proj.bias: 128
mamba.backbone.layers.0.mixer.out_proj.weight: 8192
mamba.backbone.layers.0.norm.weight: 64
mamba.backbone.layers.1.mixer.A_log: 2048
mamba.backbone.layers.1.mixer.D: 128
mamba.backbone.layers.1.mixer.in_proj.weight: 16384
mamba.backbone.layers.1.mixer.conv1d.weight: 512
mamba.backbone.layers.1.mixer.conv1d.bias: 128
mamba.backbone.layers.1.mixer.x_proj.weight: 4608
mamba.backbone.layers.1.mixer.dt_proj.weight: 512
mamba.backbone.layers.1.mixer.dt_proj.bias: 128
mamba.backbone.layers.1.mixer.out_proj.weight: 8192
mamba.backbone.layers.1.norm.weight: 64
mamba.backbone.layers.2.mixer.A_log: 2048
mamba.backbone.layers.2.mixer.D: 128
mamba.backbone.layers.2.mixer.in_proj.weight: 16384
mamba.backbone.layers.2.mixer.conv1d.weight: 512
mamba.backbone.layers.2.mixer.conv1d.bias: 128
mamba.backbone.layers.2.mixer.x_proj.weight: 4608
mamba.backbone.layers.2.mixer.dt_proj.weight: 512
mamba.backbone.layers.2.mixer.dt_proj.bias: 128
mamba.backbone.layers.2.mixer.out_proj.weight: 8192
mamba.backbone.layers.2.norm.weight: 64
mamba.backbone.layers.3.mixer.A_log: 2048
mamba.backbone.layers.3.mixer.D: 128
mamba.backbone.layers.3.mixer.in_proj.weight: 16384
mamba.backbone.layers.3.mixer.conv1d.weight: 512
mamba.backbone.layers.3.mixer.conv1d.bias: 128
mamba.backbone.layers.3.mixer.x_proj.weight: 4608
mamba.backbone.layers.3.mixer.dt_proj.weight: 512
mamba.backbone.layers.3.mixer.dt_proj.bias: 128
mamba.backbone.layers.3.mixer.out_proj.weight: 8192
mamba.backbone.layers.3.norm.weight: 64
mamba.backbone.layers.4.mixer.A_log: 2048
mamba.backbone.layers.4.mixer.D: 128
mamba.backbone.layers.4.mixer.in_proj.weight: 16384
mamba.backbone.layers.4.mixer.conv1d.weight: 512
mamba.backbone.layers.4.mixer.conv1d.bias: 128
mamba.backbone.layers.4.mixer.x_proj.weight: 4608
mamba.backbone.layers.4.mixer.dt_proj.weight: 512
mamba.backbone.layers.4.mixer.dt_proj.bias: 128
mamba.backbone.layers.4.mixer.out_proj.weight: 8192
mamba.backbone.layers.4.norm.weight: 64
mamba.backbone.layers.5.mixer.A_log: 2048
mamba.backbone.layers.5.mixer.D: 128
mamba.backbone.layers.5.mixer.in_proj.weight: 16384
mamba.backbone.layers.5.mixer.conv1d.weight: 512
mamba.backbone.layers.5.mixer.conv1d.bias: 128
mamba.backbone.layers.5.mixer.x_proj.weight: 4608
mamba.backbone.layers.5.mixer.dt_proj.weight: 512
mamba.backbone.layers.5.mixer.dt_proj.bias: 128
mamba.backbone.layers.5.mixer.out_proj.weight: 8192
mamba.backbone.layers.5.norm.weight: 64
mamba.backbone.layers.6.mixer.A_log: 2048
mamba.backbone.layers.6.mixer.D: 128
mamba.backbone.layers.6.mixer.in_proj.weight: 16384
mamba.backbone.layers.6.mixer.conv1d.weight: 512
mamba.backbone.layers.6.mixer.conv1d.bias: 128
mamba.backbone.layers.6.mixer.x_proj.weight: 4608
mamba.backbone.layers.6.mixer.dt_proj.weight: 512
mamba.backbone.layers.6.mixer.dt_proj.bias: 128
mamba.backbone.layers.6.mixer.out_proj.weight: 8192
mamba.backbone.layers.6.norm.weight: 64
mamba.backbone.layers.7.mixer.A_log: 2048
mamba.backbone.layers.7.mixer.D: 128
mamba.backbone.layers.7.mixer.in_proj.weight: 16384
mamba.backbone.layers.7.mixer.conv1d.weight: 512
mamba.backbone.layers.7.mixer.conv1d.bias: 128
mamba.backbone.layers.7.mixer.x_proj.weight: 4608
mamba.backbone.layers.7.mixer.dt_proj.weight: 512
mamba.backbone.layers.7.mixer.dt_proj.bias: 128
mamba.backbone.layers.7.mixer.out_proj.weight: 8192
mamba.backbone.layers.7.norm.weight: 64
mamba.backbone.layers.8.mixer.A_log: 2048
mamba.backbone.layers.8.mixer.D: 128
mamba.backbone.layers.8.mixer.in_proj.weight: 16384
mamba.backbone.layers.8.mixer.conv1d.weight: 512
mamba.backbone.layers.8.mixer.conv1d.bias: 128
mamba.backbone.layers.8.mixer.x_proj.weight: 4608
mamba.backbone.layers.8.mixer.dt_proj.weight: 512
mamba.backbone.layers.8.mixer.dt_proj.bias: 128
mamba.backbone.layers.8.mixer.out_proj.weight: 8192
mamba.backbone.layers.8.norm.weight: 64
mamba.backbone.layers.9.mixer.A_log: 2048
mamba.backbone.layers.9.mixer.D: 128
mamba.backbone.layers.9.mixer.in_proj.weight: 16384
mamba.backbone.layers.9.mixer.conv1d.weight: 512
mamba.backbone.layers.9.mixer.conv1d.bias: 128
mamba.backbone.layers.9.mixer.x_proj.weight: 4608
mamba.backbone.layers.9.mixer.dt_proj.weight: 512
mamba.backbone.layers.9.mixer.dt_proj.bias: 128
mamba.backbone.layers.9.mixer.out_proj.weight: 8192
mamba.backbone.layers.9.norm.weight: 64
mamba.backbone.layers.10.mixer.A_log: 2048
mamba.backbone.layers.10.mixer.D: 128
mamba.backbone.layers.10.mixer.in_proj.weight: 16384
mamba.backbone.layers.10.mixer.conv1d.weight: 512
mamba.backbone.layers.10.mixer.conv1d.bias: 128
mamba.backbone.layers.10.mixer.x_proj.weight: 4608
mamba.backbone.layers.10.mixer.dt_proj.weight: 512
mamba.backbone.layers.10.mixer.dt_proj.bias: 128
mamba.backbone.layers.10.mixer.out_proj.weight: 8192
mamba.backbone.layers.10.norm.weight: 64
mamba.backbone.layers.11.mixer.A_log: 2048
mamba.backbone.layers.11.mixer.D: 128
mamba.backbone.layers.11.mixer.in_proj.weight: 16384
mamba.backbone.layers.11.mixer.conv1d.weight: 512
mamba.backbone.layers.11.mixer.conv1d.bias: 128
mamba.backbone.layers.11.mixer.x_proj.weight: 4608
mamba.backbone.layers.11.mixer.dt_proj.weight: 512
mamba.backbone.layers.11.mixer.dt_proj.bias: 128
mamba.backbone.layers.11.mixer.out_proj.weight: 8192
mamba.backbone.layers.11.norm.weight: 64
mamba.backbone.norm_f.weight: 64
n_params=392512
ignored=[('embedder.embedder.weight', 320), ('embedder.embedder.bias', 64), ('head.head.weight', 64)]
  0%|          | 0/1000 [00:00<?, ?it/s]
[[split=train]] train_iter: 0, batch_idx: 0, input: [[0.8162900805473328, -0.8937985897064209, -1.231592059135437, -0.006909837480634451, -0.6606903672218323], [0.8797289133071899, 0.0, 0.0, 0.0, 0.0], [1.338809609413147, 1.693846344947815, 0.37473538517951965, -2.216855049133301, -1.0760955810546875], [-2.262162923812866, 0.0, 0.0, 0.0, 0.0], [0.4095987379550934, -0.959525465965271, -0.40346845984458923, 0.10880132764577866, -0.7321749329566956], [1.30717134475708, 0.0, 0.0, 0.0, 0.0], [-0.6105225086212158, -1.8265992403030396, 0.3599432110786438, 0.5522516369819641, 0.9502310156822205], [0.5753015279769897, 0.0, 0.0, 0.0, 0.0], [-1.1912099123001099, 0.7222743630409241, -0.675369381904602, -0.3983597755432129, 0.9675205945968628], [-0.4647520184516907, 0.0, 0.0, 0.0, 0.0]], target: [0.8797289133071899, -2.262162923812866, 1.30717134475708, 0.5753015279769897, -0.4647520184516907, -0.9855940937995911, 1.3876891136169434, 0.9503867626190186, -1.9472428560256958, 0.09046852588653564]
[[split=train]] train_iter: 0, batch_idx: 1, input: [[-0.4491397440433502, 1.8280136585235596, 0.9897819757461548, -0.320390909910202, 0.5379754900932312], [-3.1020119190216064, 0.0, 0.0, 0.0, 0.0], [0.0064454516395926476, 0.15073370933532715, 0.17710530757904053, 0.04110562428832054, 1.1405218839645386], [-0.8037629723548889, 0.0, 0.0, 0.0, 0.0], [0.3398081958293915, 0.08187104016542435, 0.665878176689148, 1.0534658432006836, -0.07220642268657684], [-0.6552398204803467, 0.0, 0.0, 0.0, 0.0], [-1.1079179048538208, 0.2800193727016449, 0.14736108481884003, 0.5868382453918457, -1.0011214017868042], [-0.34071919322013855, 0.0, 0.0, 0.0, 0.0], [-0.547766387462616, -1.1050153970718384, 0.925240159034729, -1.4807944297790527, 0.6986226439476013], [-1.229428768157959, 0.0, 0.0, 0.0, 0.0]], target: [-3.1020119190216064, -0.8037629723548889, -0.6552398204803467, -0.34071919322013855, -1.229428768157959, 3.2118775844573975, -2.7036781311035156, 0.0024459362030029297, -2.583981513977051, -1.1445428133010864]

  1%|          | 9/1000 [00:05<07:45,  2.13it/s]
[[split=eval]] train_iter: 0, eval_iter: 1, batch_idx: 0, input: [[0.32853397727012634, -1.113255262374878, 0.838888943195343, -0.6574654579162598, -0.3239317536354065], [1.1324061155319214, 0.0, 0.0, 0.0, 0.0], [1.142399787902832, -0.038401633501052856, 1.4896352291107178, 0.07022134214639664, -1.397045373916626], [-1.6001558303833008, 0.0, 0.0, 0.0, 0.0], [1.96773362159729, 0.31781065464019775, -1.793107509613037, 0.3199353814125061, -0.9854117631912231], [0.5978349447250366, 0.0, 0.0, 0.0, 0.0], [-1.1753662824630737, -0.7069042921066284, 1.631754994392395, 0.33515477180480957, 0.7220890522003174], [-0.8600477576255798, 0.0, 0.0, 0.0, 0.0], [-0.14661362767219543, -1.3920907974243164, -0.6477355360984802, -0.11829765141010284, -1.3703991174697876], [-0.6125239133834839, 0.0, 0.0, 0.0, 0.0]], target: [1.1324061155319214, -1.6001558303833008, 0.5978349447250366, -0.8600477576255798, -0.6125239133834839, 5.261972427368164, 3.258908748626709, 0.8352506160736084, 1.506016492843628, 2.41762113571167]







  6%|▋         | 64/1000 [00:00<00:12, 75.19it/s]
[[split=test]] train_iter: 0, eval_iter: 0, batch_idx: 0, input: [[0.5233749747276306, -1.555262804031372, -1.104850172996521, -0.22152866423130035, -0.3305017352104187], [-2.974944591522217, 0.0, 0.0, 0.0, 0.0], [-0.38427287340164185, 0.9142257571220398, -0.36574357748031616, 1.6145460605621338, 0.564517080783844], [0.30546462535858154, 0.0, 0.0, 0.0, 0.0], [1.9236397743225098, 0.42662206292152405, -0.26191943883895874, -0.714324414730072, 0.24332183599472046], [2.76631760597229, 0.0, 0.0, 0.0, 0.0], [0.9638676047325134, -0.7983613610267639, 2.106900215148926, -0.16797976195812225, 0.5600066184997559], [1.2722959518432617, 0.0, 0.0, 0.0, 0.0], [-0.6914563179016113, -0.15334463119506836, -1.0036933422088623, 1.2572879791259766, 0.28286659717559814], [-2.098421812057495, 0.0, 0.0, 0.0, 0.0]], target: [-2.974944591522217, 0.30546462535858154, 2.76631760597229, 1.2722959518432617, -2.098421812057495, 2.0986344814300537, -2.357351303100586, -1.8625391721725464, 3.390392541885376, -1.1513428688049316]
[[split=test]] train_iter: 0, eval_iter: 1, batch_idx: 0, input: [[-0.8695881366729736, -0.09257642179727554, -0.06272988021373749, 0.9176374077796936, -0.11943841725587845], [-1.8458374738693237, 0.0, 0.0, 0.0, 0.0], [-0.6221474409103394, 0.23221780359745026, -0.8437789082527161, -0.6752217411994934, -0.22602151334285736], [-0.38249754905700684, 0.0, 0.0, 0.0, 0.0], [0.7572357058525085, 2.26025128364563, 0.9596935510635376, 0.9555346965789795, -0.4482548236846924], [-3.849099636077881, 0.0, 0.0, 0.0, 0.0], [-1.8399732112884521, -0.9456486701965332, 1.5352774858474731, -0.518601655960083, 2.4590773582458496], [-1.4611384868621826, 0.0, 0.0, 0.0, 0.0], [-1.4060301780700684, -0.6523200273513794, 1.0126824378967285, 1.186187505722046, -0.49887436628341675], [-1.310933232307434, 0.0, 0.0, 0.0, 0.0]], target: [-1.8458374738693237, -0.38249754905700684, -3.849099636077881, -1.4611384868621826, -1.310933232307434, -0.5454149842262268, -4.548440933227539, -2.3483219146728516, -0.3762666881084442, 5.832878112792969]






eval-test: {'loss': 5.635309219360352, 'last_loss': 5.732820510864258, 'oracle_loss': 4.995128154754639}
patience_left: 5
best_test_loss: 5.635309219360352
best_test_loss_train_iter: 0
  0%|          | 0/100001 [00:32<?, ?it/s]
Error executing job with overrides: ['data=linear-regression', 'data.curriculum.dims.end=5', 'data.curriculum.points_train.end=32', 'data.curriculum.points_train.start=32', 'data.curriculum.points_val.end=1024', 'data.curriculum.points_val.start=1024', 'eval.every=5000', 'eval.iters=1000', 'model=mamba', 'model.d_model=64', 'model.max_seq_len=4096', 'model.n_layer=12', 'optimizer=adamw', 'optimizer.betas=[0.9, 0.95]', 'optimizer.lr=0.001', 'optimizer.weight_decay=0', 'scheduler.decay_lr=True', 'seed=9656', 'train.batch_size=128', 'train.do_early_stop=False', 'train.iters=100001', 'train.log_every=1000', 'train.parallel_loss=True']
Traceback (most recent call last):
  File "/mnt/ceph_rbd/synth-icl/main.py", line 1683, in main
    train(
  File "/mnt/ceph_rbd/synth-icl/main.py", line 1318, in train
    train_iter, exit_training, best_eval_loss, best_eval_acc = train_loop(
  File "/mnt/ceph_rbd/synth-icl/main.py", line 1072, in train_loop
    scaler.scale(loss_mean).backward()
  File "/root/anaconda3/envs/icl/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/root/anaconda3/envs/icl/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Found dtype Float but expected Half
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.