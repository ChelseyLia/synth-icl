[34m[1mwandb[39m[22m: [33mWARNING[39m Config item 'seed' was locked by 'sweep' (ignored update).
[34m[1mwandb[39m[22m: [33mWARNING[39m Config item 'optimizer' was locked by 'sweep' (ignored update).
[34m[1mwandb[39m[22m: [33mWARNING[39m Config item 'save_checkpoints' was locked by 'sweep' (ignored update).
[34m[1mwandb[39m[22m: [33mWARNING[39m Config item 'model' was locked by 'sweep' (ignored update).
[34m[1mwandb[39m[22m: [33mWARNING[39m Config item 'data' was locked by 'sweep' (ignored update).
CONFIG
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 2059
â”œâ”€â”€ seed_eval
â”‚   â””â”€â”€ 2000
â”œâ”€â”€ seed_test
â”‚   â””â”€â”€ 3000
â”œâ”€â”€ device
â”‚   â””â”€â”€ cuda
â”œâ”€â”€ train
â”‚   â””â”€â”€ do: true
â”‚       do_save: false
â”‚       reset_every: null
â”‚       dtype: float32
â”‚       compile: false
â”‚       do_amp: false
â”‚       batch_size: 50
â”‚       grad_clip: 1.0
â”‚       iters: 200001
â”‚       samples: null
â”‚       log_every: 1000
â”‚       save_every: 50000
â”‚       save_path: ./out
â”‚       num_workers: 0
â”‚       do_early_stop: false
â”‚       early_stop_patience: 5
â”‚       early_stop_metric: loss
â”‚       early_stop_tol: 0.001
â”‚       early_stop_start_iter: 40000.200000000004
â”‚       early_stop_acc: null
â”‚       parallel_loss: true
â”‚       merge_embeds: false
â”‚       merge_type: sum
â”‚
â”œâ”€â”€ eval
â”‚   â””â”€â”€ do: true
â”‚       split: both
â”‚       batch_size: 1
â”‚       every: 5000
â”‚       every_samples: null
â”‚       iters: 1000
â”‚
â”œâ”€â”€ scheduler
â”‚   â””â”€â”€ decay_lr: true
â”‚       warmup_iters: 40000.200000000004
â”‚       lr_decay_iters: 200001
â”‚       min_lr: 3.16228e-05
â”‚
â”œâ”€â”€ optimizer
â”‚   â””â”€â”€ lr: 0.000316228
â”‚       _name_: adamw
â”‚       weight_decay: 0
â”‚       betas:
â”‚       - 0.9
â”‚       - 0.95
â”‚
â”œâ”€â”€ log_level
â”‚   â””â”€â”€ info
â”œâ”€â”€ examples_to_log
â”‚   â””â”€â”€ 3
â”œâ”€â”€ log_batch_idx
â”‚   â””â”€â”€ [0, 1]
â”œâ”€â”€ wandb
â”‚   â””â”€â”€ project: icl-arch
â”‚
â”œâ”€â”€ save_checkpoints
â”‚   â””â”€â”€ True
â”œâ”€â”€ nl_icl
â”‚   â””â”€â”€ do: false
â”‚       checkpoint_path: null
â”‚       hf_path: null
â”‚       task: sentiment
â”‚       n_seeds: 10
â”‚       min_examples_per_class: 0
â”‚       max_examples_per_class: 9
â”‚       do_full_vocab: true
â”‚
â”œâ”€â”€ do_count_param_only
â”‚   â””â”€â”€ False
â”œâ”€â”€ model
â”‚   â””â”€â”€ _name_: mamba
â”‚       d_model: 800
â”‚       n_layer: 8
â”‚       norm_epsilon: 1.0e-05
â”‚       rms_norm: false
â”‚       fused_add_norm: false
â”‚       residual_in_fp32: false
â”‚       max_seq_len: 512
â”‚
â””â”€â”€ data
    â””â”€â”€ _name_: language-modeling-HF
        vocab_size: 10000
        preprocessing_num_workers: 20
        overwrite_cache: false
        tokenizer_dir: EleutherAI/gpt-neo-125M
        max_seq_len: 512
        do_shuffle: true
        version: original
        sent:
          filepath: stats/sent.csv
          examples_per_class: 3
          same_name: Lilly
          do_flip_class: false
/root/anaconda3/envs/icl/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
/root/anaconda3/envs/icl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/root/anaconda3/envs/icl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
mamba.backbone.layers.0.mixer.A_log: 25600
mamba.backbone.layers.0.mixer.D: 1600
mamba.backbone.layers.0.mixer.in_proj.weight: 2560000
mamba.backbone.layers.0.mixer.conv1d.weight: 6400
mamba.backbone.layers.0.mixer.conv1d.bias: 1600
mamba.backbone.layers.0.mixer.x_proj.weight: 131200
mamba.backbone.layers.0.mixer.dt_proj.weight: 80000
mamba.backbone.layers.0.mixer.dt_proj.bias: 1600
mamba.backbone.layers.0.mixer.out_proj.weight: 1280000
mamba.backbone.layers.0.norm.weight: 800
mamba.backbone.layers.1.mixer.A_log: 25600
mamba.backbone.layers.1.mixer.D: 1600
mamba.backbone.layers.1.mixer.in_proj.weight: 2560000
mamba.backbone.layers.1.mixer.conv1d.weight: 6400
mamba.backbone.layers.1.mixer.conv1d.bias: 1600
mamba.backbone.layers.1.mixer.x_proj.weight: 131200
mamba.backbone.layers.1.mixer.dt_proj.weight: 80000
mamba.backbone.layers.1.mixer.dt_proj.bias: 1600
mamba.backbone.layers.1.mixer.out_proj.weight: 1280000
mamba.backbone.layers.1.norm.weight: 800
mamba.backbone.layers.2.mixer.A_log: 25600
mamba.backbone.layers.2.mixer.D: 1600
mamba.backbone.layers.2.mixer.in_proj.weight: 2560000
mamba.backbone.layers.2.mixer.conv1d.weight: 6400
mamba.backbone.layers.2.mixer.conv1d.bias: 1600
mamba.backbone.layers.2.mixer.x_proj.weight: 131200
mamba.backbone.layers.2.mixer.dt_proj.weight: 80000
mamba.backbone.layers.2.mixer.dt_proj.bias: 1600
mamba.backbone.layers.2.mixer.out_proj.weight: 1280000
mamba.backbone.layers.2.norm.weight: 800
mamba.backbone.layers.3.mixer.A_log: 25600
mamba.backbone.layers.3.mixer.D: 1600
mamba.backbone.layers.3.mixer.in_proj.weight: 2560000
mamba.backbone.layers.3.mixer.conv1d.weight: 6400
mamba.backbone.layers.3.mixer.conv1d.bias: 1600
mamba.backbone.layers.3.mixer.x_proj.weight: 131200
mamba.backbone.layers.3.mixer.dt_proj.weight: 80000
mamba.backbone.layers.3.mixer.dt_proj.bias: 1600
mamba.backbone.layers.3.mixer.out_proj.weight: 1280000
mamba.backbone.layers.3.norm.weight: 800
mamba.backbone.layers.4.mixer.A_log: 25600
mamba.backbone.layers.4.mixer.D: 1600
mamba.backbone.layers.4.mixer.in_proj.weight: 2560000
mamba.backbone.layers.4.mixer.conv1d.weight: 6400
mamba.backbone.layers.4.mixer.conv1d.bias: 1600
mamba.backbone.layers.4.mixer.x_proj.weight: 131200
mamba.backbone.layers.4.mixer.dt_proj.weight: 80000
mamba.backbone.layers.4.mixer.dt_proj.bias: 1600
mamba.backbone.layers.4.mixer.out_proj.weight: 1280000
mamba.backbone.layers.4.norm.weight: 800
mamba.backbone.layers.5.mixer.A_log: 25600
mamba.backbone.layers.5.mixer.D: 1600
mamba.backbone.layers.5.mixer.in_proj.weight: 2560000
mamba.backbone.layers.5.mixer.conv1d.weight: 6400
mamba.backbone.layers.5.mixer.conv1d.bias: 1600
mamba.backbone.layers.5.mixer.x_proj.weight: 131200
mamba.backbone.layers.5.mixer.dt_proj.weight: 80000
mamba.backbone.layers.5.mixer.dt_proj.bias: 1600
mamba.backbone.layers.5.mixer.out_proj.weight: 1280000
mamba.backbone.layers.5.norm.weight: 800
mamba.backbone.layers.6.mixer.A_log: 25600
mamba.backbone.layers.6.mixer.D: 1600
mamba.backbone.layers.6.mixer.in_proj.weight: 2560000
mamba.backbone.layers.6.mixer.conv1d.weight: 6400
mamba.backbone.layers.6.mixer.conv1d.bias: 1600
mamba.backbone.layers.6.mixer.x_proj.weight: 131200
mamba.backbone.layers.6.mixer.dt_proj.weight: 80000
mamba.backbone.layers.6.mixer.dt_proj.bias: 1600
mamba.backbone.layers.6.mixer.out_proj.weight: 1280000
mamba.backbone.layers.6.norm.weight: 800
mamba.backbone.layers.7.mixer.A_log: 25600
mamba.backbone.layers.7.mixer.D: 1600
mamba.backbone.layers.7.mixer.in_proj.weight: 2560000
mamba.backbone.layers.7.mixer.conv1d.weight: 6400
mamba.backbone.layers.7.mixer.conv1d.bias: 1600
mamba.backbone.layers.7.mixer.x_proj.weight: 131200
mamba.backbone.layers.7.mixer.dt_proj.weight: 80000
mamba.backbone.layers.7.mixer.dt_proj.bias: 1600
mamba.backbone.layers.7.mixer.out_proj.weight: 1280000
mamba.backbone.layers.7.norm.weight: 800
mamba.backbone.norm_f.weight: 800
n_params=32711200
ignored=[('embedder.embedder.weight', 40205600), ('head.head.weight', 40205600)]
  0%|                                                                                                                    | 0/200001 [00:00<?, ?it/s]
Shuffled dataset.















Grouping texts in chunks of 513 (num_proc=20): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2113776/2119719 [00:33<00:00, 69146.95 examples/s]
[[split=train]] train_iter: 0, batch_idx: 0, input: [33, 11369, 290, 22162, 547, 2712, 287, 262, 3952, 13], target: [11369, 290, 22162, 547, 2712, 287, 262, 3952, 13, 22162]
/root/anaconda3/envs/icl/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
  0%|                                                                                                                      | 0/1000 [00:00<?, ?it/s]

Grouping texts in chunks of 513 (num_proc=20): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21990/21990 [00:06<00:00, 5741.71 examples/s]
[[split=eval]] train_iter: 0, eval_iter: 0, batch_idx: 0, input: [32565, 13, 15899, 2497, 262, 22441, 1097, 290, 531, 11], target: [13, 15899, 2497, 262, 22441, 1097, 290, 531, 11, 366]
[[split=eval]] train_iter: 0, eval_iter: 1, batch_idx: 0, input: [287, 5436, 13, 7454, 2402, 257, 640, 11, 612, 373], target: [5436, 13, 7454, 2402, 257, 640, 11, 612, 373, 257]




 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 871/1000 [00:25<00:01, 125.49it/s]
[[split=test]] train_iter: 0, eval_iter: 0, batch_idx: 0, input: [43, 6548, 447, 247, 82, 4771, 8566, 24178, 13, 35134], target: [43, 6548, 447, 247, 82, 4771, 8566, 24178, 13, 35134]
[[split=test]] train_iter: 0, eval_iter: 1, batch_idx: 0, input: [43, 6548, 925, 257, 6450, 18676, 13, 35134, 318, 3772], target: [43, 6548, 925, 257, 6450, 18676, 13, 35134, 318, 3772]


 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                               | 111/1000 [00:00<00:07, 120.28it/s]
[[split=train]] train_iter: 1, batch_idx: 0, input: [10719, 284, 1064, 530, 326, 373, 517, 1611, 290, 8030], target: [284, 1064, 530, 326, 373, 517, 1611, 290, 8030, 13]

  0%|                                                                                                       | 1/200001 [01:08<3794:17:05, 68.30s/it]
[[split=train]] train_iter: 2, batch_idx: 0, input: [3114, 379, 1123, 584, 290, 531, 11, 366, 14385, 526], target: [379, 1123, 584, 290, 531, 11, 366, 14385, 526, 198]













  0%|                                                                                                        | 90/200001 [01:33<15:46:43,  3.52it/s]