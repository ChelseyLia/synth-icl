[34m[1mwandb[39m[22m: [33mWARNING[39m Config item 'seed' was locked by 'sweep' (ignored update).
[34m[1mwandb[39m[22m: [33mWARNING[39m Config item 'optimizer' was locked by 'sweep' (ignored update).
[34m[1mwandb[39m[22m: [33mWARNING[39m Config item 'model' was locked by 'sweep' (ignored update).
[34m[1mwandb[39m[22m: [33mWARNING[39m Config item 'data' was locked by 'sweep' (ignored update).
CONFIG
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 2059
â”œâ”€â”€ seed_eval
â”‚   â””â”€â”€ 2000
â”œâ”€â”€ seed_test
â”‚   â””â”€â”€ 3000
â”œâ”€â”€ device
â”‚   â””â”€â”€ cuda
â”œâ”€â”€ train
â”‚   â””â”€â”€ do: true
â”‚       do_save: false
â”‚       reset_every: null
â”‚       dtype: float32
â”‚       compile: false
â”‚       do_amp: false
â”‚       batch_size: 128
â”‚       grad_clip: 1.0
â”‚       iters: 100001
â”‚       samples: null
â”‚       log_every: 1000
â”‚       save_every: 100000
â”‚       save_path: ./out
â”‚       num_workers: 4
â”‚       do_early_stop: false
â”‚       early_stop_patience: 5
â”‚       early_stop_metric: loss
â”‚       early_stop_tol: 0.001
â”‚       early_stop_start_iter: 20000.2
â”‚       early_stop_acc: null
â”‚       parallel_loss: false
â”‚       merge_embeds: false
â”‚       merge_type: sum
â”‚
â”œâ”€â”€ eval
â”‚   â””â”€â”€ do: true
â”‚       split: both
â”‚       batch_size: 1
â”‚       every: 5000
â”‚       every_samples: null
â”‚       iters: 1000
â”‚
â”œâ”€â”€ scheduler
â”‚   â””â”€â”€ decay_lr: true
â”‚       warmup_iters: 20000.2
â”‚       lr_decay_iters: 100001
â”‚       min_lr: 1.0e-05
â”‚
â”œâ”€â”€ optimizer
â”‚   â””â”€â”€ lr: 0.0001
â”‚       _name_: adamw
â”‚       weight_decay: 0
â”‚       betas:
â”‚       - 0.9
â”‚       - 0.95
â”‚
â”œâ”€â”€ log_level
â”‚   â””â”€â”€ info
â”œâ”€â”€ examples_to_log
â”‚   â””â”€â”€ 3
â”œâ”€â”€ log_batch_idx
â”‚   â””â”€â”€ [0, 1]
â”œâ”€â”€ wandb
â”‚   â””â”€â”€ project: icl-arch
â”‚
â”œâ”€â”€ save_checkpoints
â”‚   â””â”€â”€ False
â”œâ”€â”€ nl_icl
â”‚   â””â”€â”€ do: false
â”‚       checkpoint_path: null
â”‚       hf_path: null
â”‚       task: sentiment
â”‚       n_seeds: 10
â”‚       min_examples_per_class: 0
â”‚       max_examples_per_class: 9
â”‚       do_full_vocab: true
â”‚
â”œâ”€â”€ do_count_param_only
â”‚   â””â”€â”€ False
â”œâ”€â”€ model
â”‚   â””â”€â”€ _name_: mamba
â”‚       d_model: 64
â”‚       n_layer: 12
â”‚       norm_epsilon: 1.0e-05
â”‚       rms_norm: false
â”‚       fused_add_norm: false
â”‚       residual_in_fp32: false
â”‚       max_seq_len: 64
â”‚
â”œâ”€â”€ embedder
â”‚   â””â”€â”€ example_encoding: resnet
â”‚       flatten_superpixels: false
â”‚       example_dropout_prob: 0.0
â”‚       concatenate_labels: false
â”‚       use_positional_encodings: false
â”‚       positional_dropout_prob: 0.0
â”‚
â””â”€â”€ data
    â””â”€â”€ _name_: omniglot
        num_classes: null
        train_seqs: bursty
        eval_seqs: fewshot_holdout
        example_type: omniglot
        generator_config:
          n_rare_classes: 1603
          n_common_classes: 10
          n_holdout_classes: 10
          zipf_exponent: 0.0
          use_zipf_for_common_rare: false
          noise_scale: 0.0
          preserve_ordering_every_n: null
        omniglot_config:
          omniglot_split: all
          exemplars: all
          augment_images: false
        symbolic_config:
          dataset_size: 1000
        seq_config:
          seq_len: 9
          fs_shots: 4
          bursty_shots: 3
          ways: 2
          p_bursty: 1
          p_bursty_common: 0.0
          p_bursty_zipfian: 1.0
          p_fewshot: 0.1
          non_bursty_type: zipfian
          labeling_common: ordered
          labeling_rare: ordered
          randomly_generate_rare: false
          grouped: false
Files already downloaded and verified
Files already downloaded and verified
/root/anaconda3/envs/icl/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
mamba.backbone.layers.0.mixer.A_log: 2048
mamba.backbone.layers.0.mixer.D: 128
mamba.backbone.layers.0.mixer.in_proj.weight: 16384
mamba.backbone.layers.0.mixer.conv1d.weight: 512
mamba.backbone.layers.0.mixer.conv1d.bias: 128
mamba.backbone.layers.0.mixer.x_proj.weight: 4608
mamba.backbone.layers.0.mixer.dt_proj.weight: 512
mamba.backbone.layers.0.mixer.dt_proj.bias: 128
mamba.backbone.layers.0.mixer.out_proj.weight: 8192
mamba.backbone.layers.0.norm.weight: 64
mamba.backbone.layers.1.mixer.A_log: 2048
mamba.backbone.layers.1.mixer.D: 128
mamba.backbone.layers.1.mixer.in_proj.weight: 16384
mamba.backbone.layers.1.mixer.conv1d.weight: 512
mamba.backbone.layers.1.mixer.conv1d.bias: 128
mamba.backbone.layers.1.mixer.x_proj.weight: 4608
mamba.backbone.layers.1.mixer.dt_proj.weight: 512
mamba.backbone.layers.1.mixer.dt_proj.bias: 128
mamba.backbone.layers.1.mixer.out_proj.weight: 8192
mamba.backbone.layers.1.norm.weight: 64
mamba.backbone.layers.2.mixer.A_log: 2048
mamba.backbone.layers.2.mixer.D: 128
mamba.backbone.layers.2.mixer.in_proj.weight: 16384
mamba.backbone.layers.2.mixer.conv1d.weight: 512
mamba.backbone.layers.2.mixer.conv1d.bias: 128
mamba.backbone.layers.2.mixer.x_proj.weight: 4608
mamba.backbone.layers.2.mixer.dt_proj.weight: 512
mamba.backbone.layers.2.mixer.dt_proj.bias: 128
mamba.backbone.layers.2.mixer.out_proj.weight: 8192
mamba.backbone.layers.2.norm.weight: 64
mamba.backbone.layers.3.mixer.A_log: 2048
mamba.backbone.layers.3.mixer.D: 128
mamba.backbone.layers.3.mixer.in_proj.weight: 16384
mamba.backbone.layers.3.mixer.conv1d.weight: 512
mamba.backbone.layers.3.mixer.conv1d.bias: 128
mamba.backbone.layers.3.mixer.x_proj.weight: 4608
mamba.backbone.layers.3.mixer.dt_proj.weight: 512
mamba.backbone.layers.3.mixer.dt_proj.bias: 128
mamba.backbone.layers.3.mixer.out_proj.weight: 8192
mamba.backbone.layers.3.norm.weight: 64
mamba.backbone.layers.4.mixer.A_log: 2048
mamba.backbone.layers.4.mixer.D: 128
mamba.backbone.layers.4.mixer.in_proj.weight: 16384
mamba.backbone.layers.4.mixer.conv1d.weight: 512
mamba.backbone.layers.4.mixer.conv1d.bias: 128
mamba.backbone.layers.4.mixer.x_proj.weight: 4608
mamba.backbone.layers.4.mixer.dt_proj.weight: 512
mamba.backbone.layers.4.mixer.dt_proj.bias: 128
mamba.backbone.layers.4.mixer.out_proj.weight: 8192
mamba.backbone.layers.4.norm.weight: 64
mamba.backbone.layers.5.mixer.A_log: 2048
mamba.backbone.layers.5.mixer.D: 128
mamba.backbone.layers.5.mixer.in_proj.weight: 16384
mamba.backbone.layers.5.mixer.conv1d.weight: 512
mamba.backbone.layers.5.mixer.conv1d.bias: 128
mamba.backbone.layers.5.mixer.x_proj.weight: 4608
mamba.backbone.layers.5.mixer.dt_proj.weight: 512
mamba.backbone.layers.5.mixer.dt_proj.bias: 128
mamba.backbone.layers.5.mixer.out_proj.weight: 8192
mamba.backbone.layers.5.norm.weight: 64
mamba.backbone.layers.6.mixer.A_log: 2048
mamba.backbone.layers.6.mixer.D: 128
mamba.backbone.layers.6.mixer.in_proj.weight: 16384
mamba.backbone.layers.6.mixer.conv1d.weight: 512
mamba.backbone.layers.6.mixer.conv1d.bias: 128
mamba.backbone.layers.6.mixer.x_proj.weight: 4608
mamba.backbone.layers.6.mixer.dt_proj.weight: 512
mamba.backbone.layers.6.mixer.dt_proj.bias: 128
mamba.backbone.layers.6.mixer.out_proj.weight: 8192
mamba.backbone.layers.6.norm.weight: 64
mamba.backbone.layers.7.mixer.A_log: 2048
mamba.backbone.layers.7.mixer.D: 128
mamba.backbone.layers.7.mixer.in_proj.weight: 16384
mamba.backbone.layers.7.mixer.conv1d.weight: 512
mamba.backbone.layers.7.mixer.conv1d.bias: 128
mamba.backbone.layers.7.mixer.x_proj.weight: 4608
mamba.backbone.layers.7.mixer.dt_proj.weight: 512
mamba.backbone.layers.7.mixer.dt_proj.bias: 128
mamba.backbone.layers.7.mixer.out_proj.weight: 8192
mamba.backbone.layers.7.norm.weight: 64
mamba.backbone.layers.8.mixer.A_log: 2048
mamba.backbone.layers.8.mixer.D: 128
mamba.backbone.layers.8.mixer.in_proj.weight: 16384
mamba.backbone.layers.8.mixer.conv1d.weight: 512
mamba.backbone.layers.8.mixer.conv1d.bias: 128
mamba.backbone.layers.8.mixer.x_proj.weight: 4608
mamba.backbone.layers.8.mixer.dt_proj.weight: 512
mamba.backbone.layers.8.mixer.dt_proj.bias: 128
mamba.backbone.layers.8.mixer.out_proj.weight: 8192
mamba.backbone.layers.8.norm.weight: 64
mamba.backbone.layers.9.mixer.A_log: 2048
mamba.backbone.layers.9.mixer.D: 128
mamba.backbone.layers.9.mixer.in_proj.weight: 16384
mamba.backbone.layers.9.mixer.conv1d.weight: 512
mamba.backbone.layers.9.mixer.conv1d.bias: 128
mamba.backbone.layers.9.mixer.x_proj.weight: 4608
mamba.backbone.layers.9.mixer.dt_proj.weight: 512
mamba.backbone.layers.9.mixer.dt_proj.bias: 128
mamba.backbone.layers.9.mixer.out_proj.weight: 8192
mamba.backbone.layers.9.norm.weight: 64
mamba.backbone.layers.10.mixer.A_log: 2048
mamba.backbone.layers.10.mixer.D: 128
mamba.backbone.layers.10.mixer.in_proj.weight: 16384
mamba.backbone.layers.10.mixer.conv1d.weight: 512
mamba.backbone.layers.10.mixer.conv1d.bias: 128
mamba.backbone.layers.10.mixer.x_proj.weight: 4608
mamba.backbone.layers.10.mixer.dt_proj.weight: 512
mamba.backbone.layers.10.mixer.dt_proj.bias: 128
mamba.backbone.layers.10.mixer.out_proj.weight: 8192
mamba.backbone.layers.10.norm.weight: 64
mamba.backbone.layers.11.mixer.A_log: 2048
mamba.backbone.layers.11.mixer.D: 128
mamba.backbone.layers.11.mixer.in_proj.weight: 16384
mamba.backbone.layers.11.mixer.conv1d.weight: 512
mamba.backbone.layers.11.mixer.conv1d.bias: 128
mamba.backbone.layers.11.mixer.x_proj.weight: 4608
mamba.backbone.layers.11.mixer.dt_proj.weight: 512
mamba.backbone.layers.11.mixer.dt_proj.bias: 128
mamba.backbone.layers.11.mixer.out_proj.weight: 8192
mamba.backbone.layers.11.norm.weight: 64
mamba.backbone.norm_f.weight: 64
n_params=392512
ignored=[('embedder.example_encoding.embedder.embedder.convolution.weight', 3136), ('embedder.example_encoding.embedder.embedder.normalization.weight', 64), ('embedder.example_encoding.embedder.embedder.normalization.bias', 64), ('embedder.example_encoding.encoder.stages.0.layers.0.shortcut.convolution.weight', 1024), ('embedder.example_encoding.encoder.stages.0.layers.0.shortcut.normalization.weight', 16), ('embedder.example_encoding.encoder.stages.0.layers.0.shortcut.normalization.bias', 16), ('embedder.example_encoding.encoder.stages.0.layers.0.layer.0.convolution.weight', 9216), ('embedder.example_encoding.encoder.stages.0.layers.0.layer.0.normalization.weight', 16), ('embedder.example_encoding.encoder.stages.0.layers.0.layer.0.normalization.bias', 16), ('embedder.example_encoding.encoder.stages.0.layers.0.layer.1.convolution.weight', 2304), ('embedder.example_encoding.encoder.stages.0.layers.0.layer.1.normalization.weight', 16), ('embedder.example_encoding.encoder.stages.0.layers.0.layer.1.normalization.bias', 16), ('embedder.example_encoding.encoder.stages.0.layers.1.layer.0.convolution.weight', 2304), ('embedder.example_encoding.encoder.stages.0.layers.1.layer.0.normalization.weight', 16), ('embedder.example_encoding.encoder.stages.0.layers.1.layer.0.normalization.bias', 16), ('embedder.example_encoding.encoder.stages.0.layers.1.layer.1.convolution.weight', 2304), ('embedder.example_encoding.encoder.stages.0.layers.1.layer.1.normalization.weight', 16), ('embedder.example_encoding.encoder.stages.0.layers.1.layer.1.normalization.bias', 16), ('embedder.example_encoding.encoder.stages.1.layers.0.shortcut.convolution.weight', 512), ('embedder.example_encoding.encoder.stages.1.layers.0.shortcut.normalization.weight', 32), ('embedder.example_encoding.encoder.stages.1.layers.0.shortcut.normalization.bias', 32), ('embedder.example_encoding.encoder.stages.1.layers.0.layer.0.convolution.weight', 4608), ('embedder.example_encoding.encoder.stages.1.layers.0.layer.0.normalization.weight', 32), ('embedder.example_encoding.encoder.stages.1.layers.0.layer.0.normalization.bias', 32), ('embedder.example_encoding.encoder.stages.1.layers.0.layer.1.convolution.weight', 9216), ('embedder.example_encoding.encoder.stages.1.layers.0.layer.1.normalization.weight', 32), ('embedder.example_encoding.encoder.stages.1.layers.0.layer.1.normalization.bias', 32), ('embedder.example_encoding.encoder.stages.1.layers.1.layer.0.convolution.weight', 9216), ('embedder.example_encoding.encoder.stages.1.layers.1.layer.0.normalization.weight', 32), ('embedder.example_encoding.encoder.stages.1.layers.1.layer.0.normalization.bias', 32), ('embedder.example_encoding.encoder.stages.1.layers.1.layer.1.convolution.weight', 9216), ('embedder.example_encoding.encoder.stages.1.layers.1.layer.1.normalization.weight', 32), ('embedder.example_encoding.encoder.stages.1.layers.1.layer.1.normalization.bias', 32), ('embedder.example_encoding.encoder.stages.2.layers.0.shortcut.convolution.weight', 1024), ('embedder.example_encoding.encoder.stages.2.layers.0.shortcut.normalization.weight', 32), ('embedder.example_encoding.encoder.stages.2.layers.0.shortcut.normalization.bias', 32), ('embedder.example_encoding.encoder.stages.2.layers.0.layer.0.convolution.weight', 9216), ('embedder.example_encoding.encoder.stages.2.layers.0.layer.0.normalization.weight', 32), ('embedder.example_encoding.encoder.stages.2.layers.0.layer.0.normalization.bias', 32), ('embedder.example_encoding.encoder.stages.2.layers.0.layer.1.convolution.weight', 9216), ('embedder.example_encoding.encoder.stages.2.layers.0.layer.1.normalization.weight', 32), ('embedder.example_encoding.encoder.stages.2.layers.0.layer.1.normalization.bias', 32), ('embedder.example_encoding.encoder.stages.2.layers.1.layer.0.convolution.weight', 9216), ('embedder.example_encoding.encoder.stages.2.layers.1.layer.0.normalization.weight', 32), ('embedder.example_encoding.encoder.stages.2.layers.1.layer.0.normalization.bias', 32), ('embedder.example_encoding.encoder.stages.2.layers.1.layer.1.convolution.weight', 9216), ('embedder.example_encoding.encoder.stages.2.layers.1.layer.1.normalization.weight', 32), ('embedder.example_encoding.encoder.stages.2.layers.1.layer.1.normalization.bias', 32), ('embedder.example_encoding.encoder.stages.3.layers.0.shortcut.convolution.weight', 2048), ('embedder.example_encoding.encoder.stages.3.layers.0.shortcut.normalization.weight', 64), ('embedder.example_encoding.encoder.stages.3.layers.0.shortcut.normalization.bias', 64), ('embedder.example_encoding.encoder.stages.3.layers.0.layer.0.convolution.weight', 18432), ('embedder.example_encoding.encoder.stages.3.layers.0.layer.0.normalization.weight', 64), ('embedder.example_encoding.encoder.stages.3.layers.0.layer.0.normalization.bias', 64), ('embedder.example_encoding.encoder.stages.3.layers.0.layer.1.convolution.weight', 36864), ('embedder.example_encoding.encoder.stages.3.layers.0.layer.1.normalization.weight', 64), ('embedder.example_encoding.encoder.stages.3.layers.0.layer.1.normalization.bias', 64), ('embedder.example_encoding.encoder.stages.3.layers.1.layer.0.convolution.weight', 36864), ('embedder.example_encoding.encoder.stages.3.layers.1.layer.0.normalization.weight', 64), ('embedder.example_encoding.encoder.stages.3.layers.1.layer.0.normalization.bias', 64), ('embedder.example_encoding.encoder.stages.3.layers.1.layer.1.convolution.weight', 36864), ('embedder.example_encoding.encoder.stages.3.layers.1.layer.1.normalization.weight', 64), ('embedder.example_encoding.encoder.stages.3.layers.1.layer.1.normalization.bias', 64), ('embedder.embs.weight', 103872), ('head.fc.weight', 103872), ('head.fc.bias', 1623)]
  0%|                                                                                                                                                                 | 0/100001 [00:00<?, ?it/s]
  0%|                                                                                                                                                                   | 0/1000 [00:00<?, ?it/s]

  0%|â–                                                                                                                                                        | 1/1000 [00:07<2:12:50,  7.98s/it]

  0%|â–Ž                                                                                                                                                        | 2/1000 [00:25<3:43:10, 13.42s/it]