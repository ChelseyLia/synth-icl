[34m[1mwandb[39m[22m: [33mWARNING[39m Config item 'seed' was locked by 'sweep' (ignored update).
[34m[1mwandb[39m[22m: [33mWARNING[39m Config item 'optimizer' was locked by 'sweep' (ignored update).
[34m[1mwandb[39m[22m: [33mWARNING[39m Config item 'model' was locked by 'sweep' (ignored update).
[34m[1mwandb[39m[22m: [33mWARNING[39m Config item 'data' was locked by 'sweep' (ignored update).
CONFIG
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 9656
â”œâ”€â”€ seed_eval
â”‚   â””â”€â”€ 2000
â”œâ”€â”€ seed_test
â”‚   â””â”€â”€ 3000
â”œâ”€â”€ device
â”‚   â””â”€â”€ cuda
â”œâ”€â”€ train
â”‚   â””â”€â”€ do: true
â”‚       do_save: false
â”‚       reset_every: null
â”‚       dtype: float32
â”‚       compile: false
â”‚       do_amp: false
â”‚       batch_size: 128
â”‚       grad_clip: 1.0
â”‚       iters: 100001
â”‚       samples: null
â”‚       log_every: 1000
â”‚       save_every: 100000
â”‚       save_path: ./out
â”‚       num_workers: 0
â”‚       do_early_stop: false
â”‚       early_stop_patience: 5
â”‚       early_stop_metric: loss
â”‚       early_stop_tol: 0.001
â”‚       early_stop_start_iter: 20000.2
â”‚       early_stop_acc: null
â”‚       parallel_loss: true
â”‚       merge_embeds: false
â”‚       merge_type: sum
â”‚
â”œâ”€â”€ eval
â”‚   â””â”€â”€ do: true
â”‚       split: both
â”‚       batch_size: 1
â”‚       every: 5000
â”‚       every_samples: null
â”‚       iters: 1000
â”‚
â”œâ”€â”€ scheduler
â”‚   â””â”€â”€ decay_lr: true
â”‚       warmup_iters: 20000.2
â”‚       lr_decay_iters: 100001
â”‚       min_lr: 0.0001
â”‚
â”œâ”€â”€ optimizer
â”‚   â””â”€â”€ lr: 0.001
â”‚       _name_: adamw
â”‚       weight_decay: 0
â”‚       betas:
â”‚       - 0.9
â”‚       - 0.95
â”‚
â”œâ”€â”€ log_level
â”‚   â””â”€â”€ info
â”œâ”€â”€ examples_to_log
â”‚   â””â”€â”€ 3
â”œâ”€â”€ log_batch_idx
â”‚   â””â”€â”€ [0, 1]
â”œâ”€â”€ wandb
â”‚   â””â”€â”€ project: icl-arch
â”‚
â”œâ”€â”€ save_checkpoints
â”‚   â””â”€â”€ False
â”œâ”€â”€ nl_icl
â”‚   â””â”€â”€ do: false
â”‚       checkpoint_path: null
â”‚       hf_path: null
â”‚       task: sentiment
â”‚       n_seeds: 10
â”‚       min_examples_per_class: 0
â”‚       max_examples_per_class: 9
â”‚       do_full_vocab: true
â”‚
â”œâ”€â”€ do_count_param_only
â”‚   â””â”€â”€ False
â”œâ”€â”€ model
â”‚   â””â”€â”€ _name_: lightconv
â”‚       d_model: 64
â”‚       d_inner: 256
â”‚       n_encoding_layer: 7
â”‚       n_decoding_layer: 12
â”‚       n_heads: 2.0
â”‚       n_kv_heads: 2.0
â”‚       max_seq_len: 4096
â”‚       conv_type: lightweight
â”‚       weight_softmax: true
â”‚       n_layer: 12
â”‚       decoder_only: true
â”‚
â””â”€â”€ data
    â””â”€â”€ _name_: linear-regression
        curriculum:
          dims:
            start: 5
            end: 5
            inc: 1
            interval: 2000
          points_train:
            start: 32
            end: 32
            inc: 2
            interval: 2000
          points_val:
            start: 1024
            end: 1024
            inc: 2
            interval: 2000
        task: linear_regression
        data: gaussian
        task_kwargs: {}
        n_dims: 5
        train_noise: 0
        val_noise: 0
Current decoder type lightweight
No module named 'lightconv_cuda'
No module named 'lightconv_cuda'
No module named 'lightconv_cuda'
No module named 'lightconv_cuda'
No module named 'lightconv_cuda'
No module named 'lightconv_cuda'
No module named 'lightconv_cuda'
No module named 'lightconv_cuda'
No module named 'lightconv_cuda'
No module named 'lightconv_cuda'
No module named 'lightconv_cuda'
No module named 'lightconv_cuda'
model.decoder.layers.0.linear1.weight: 8192
model.decoder.layers.0.linear1.bias: 128
model.decoder.layers.0.conv.weight: 6
model.decoder.layers.0.linear2.weight: 4096
model.decoder.layers.0.linear2.bias: 64
model.decoder.layers.0.conv_layer_norm.weight: 64
model.decoder.layers.0.conv_layer_norm.bias: 64
model.decoder.layers.0.fc1.weight: 16384
model.decoder.layers.0.fc1.bias: 256
model.decoder.layers.0.fc2.weight: 16384
model.decoder.layers.0.fc2.bias: 64
model.decoder.layers.0.final_layer_norm.weight: 64
model.decoder.layers.0.final_layer_norm.bias: 64
model.decoder.layers.1.linear1.weight: 8192
model.decoder.layers.1.linear1.bias: 128
model.decoder.layers.1.conv.weight: 14
model.decoder.layers.1.linear2.weight: 4096
model.decoder.layers.1.linear2.bias: 64
model.decoder.layers.1.conv_layer_norm.weight: 64
model.decoder.layers.1.conv_layer_norm.bias: 64
model.decoder.layers.1.fc1.weight: 16384
model.decoder.layers.1.fc1.bias: 256
model.decoder.layers.1.fc2.weight: 16384
model.decoder.layers.1.fc2.bias: 64
model.decoder.layers.1.final_layer_norm.weight: 64
model.decoder.layers.1.final_layer_norm.bias: 64
model.decoder.layers.2.linear1.weight: 8192
model.decoder.layers.2.linear1.bias: 128
model.decoder.layers.2.conv.weight: 30
model.decoder.layers.2.linear2.weight: 4096
model.decoder.layers.2.linear2.bias: 64
model.decoder.layers.2.conv_layer_norm.weight: 64
model.decoder.layers.2.conv_layer_norm.bias: 64
model.decoder.layers.2.fc1.weight: 16384
model.decoder.layers.2.fc1.bias: 256
model.decoder.layers.2.fc2.weight: 16384
model.decoder.layers.2.fc2.bias: 64
model.decoder.layers.2.final_layer_norm.weight: 64
model.decoder.layers.2.final_layer_norm.bias: 64
model.decoder.layers.3.linear1.weight: 8192
model.decoder.layers.3.linear1.bias: 128
model.decoder.layers.3.conv.weight: 62
model.decoder.layers.3.linear2.weight: 4096
model.decoder.layers.3.linear2.bias: 64
model.decoder.layers.3.conv_layer_norm.weight: 64
model.decoder.layers.3.conv_layer_norm.bias: 64
model.decoder.layers.3.fc1.weight: 16384
model.decoder.layers.3.fc1.bias: 256
model.decoder.layers.3.fc2.weight: 16384
model.decoder.layers.3.fc2.bias: 64
model.decoder.layers.3.final_layer_norm.weight: 64
model.decoder.layers.3.final_layer_norm.bias: 64
model.decoder.layers.4.linear1.weight: 8192
model.decoder.layers.4.linear1.bias: 128
model.decoder.layers.4.conv.weight: 62
model.decoder.layers.4.linear2.weight: 4096
model.decoder.layers.4.linear2.bias: 64
model.decoder.layers.4.conv_layer_norm.weight: 64
model.decoder.layers.4.conv_layer_norm.bias: 64
model.decoder.layers.4.fc1.weight: 16384
model.decoder.layers.4.fc1.bias: 256
model.decoder.layers.4.fc2.weight: 16384
model.decoder.layers.4.fc2.bias: 64
model.decoder.layers.4.final_layer_norm.weight: 64
model.decoder.layers.4.final_layer_norm.bias: 64
model.decoder.layers.5.linear1.weight: 8192
model.decoder.layers.5.linear1.bias: 128
model.decoder.layers.5.conv.weight: 62
model.decoder.layers.5.linear2.weight: 4096
model.decoder.layers.5.linear2.bias: 64
model.decoder.layers.5.conv_layer_norm.weight: 64
model.decoder.layers.5.conv_layer_norm.bias: 64
model.decoder.layers.5.fc1.weight: 16384
model.decoder.layers.5.fc1.bias: 256
model.decoder.layers.5.fc2.weight: 16384
model.decoder.layers.5.fc2.bias: 64
model.decoder.layers.5.final_layer_norm.weight: 64
model.decoder.layers.5.final_layer_norm.bias: 64
model.decoder.layers.6.linear1.weight: 8192
model.decoder.layers.6.linear1.bias: 128
model.decoder.layers.6.conv.weight: 62
model.decoder.layers.6.linear2.weight: 4096
model.decoder.layers.6.linear2.bias: 64
model.decoder.layers.6.conv_layer_norm.weight: 64
model.decoder.layers.6.conv_layer_norm.bias: 64
model.decoder.layers.6.fc1.weight: 16384
model.decoder.layers.6.fc1.bias: 256
model.decoder.layers.6.fc2.weight: 16384
model.decoder.layers.6.fc2.bias: 64
model.decoder.layers.6.final_layer_norm.weight: 64
model.decoder.layers.6.final_layer_norm.bias: 64
model.decoder.layers.7.linear1.weight: 8192
model.decoder.layers.7.linear1.bias: 128
model.decoder.layers.7.conv.weight: 62
model.decoder.layers.7.linear2.weight: 4096
model.decoder.layers.7.linear2.bias: 64
model.decoder.layers.7.conv_layer_norm.weight: 64
model.decoder.layers.7.conv_layer_norm.bias: 64
model.decoder.layers.7.fc1.weight: 16384
model.decoder.layers.7.fc1.bias: 256
model.decoder.layers.7.fc2.weight: 16384
model.decoder.layers.7.fc2.bias: 64
model.decoder.layers.7.final_layer_norm.weight: 64
model.decoder.layers.7.final_layer_norm.bias: 64
model.decoder.layers.8.linear1.weight: 8192
model.decoder.layers.8.linear1.bias: 128
model.decoder.layers.8.conv.weight: 62
model.decoder.layers.8.linear2.weight: 4096
model.decoder.layers.8.linear2.bias: 64
model.decoder.layers.8.conv_layer_norm.weight: 64
model.decoder.layers.8.conv_layer_norm.bias: 64
model.decoder.layers.8.fc1.weight: 16384
model.decoder.layers.8.fc1.bias: 256
model.decoder.layers.8.fc2.weight: 16384
model.decoder.layers.8.fc2.bias: 64
model.decoder.layers.8.final_layer_norm.weight: 64
model.decoder.layers.8.final_layer_norm.bias: 64
model.decoder.layers.9.linear1.weight: 8192
model.decoder.layers.9.linear1.bias: 128
model.decoder.layers.9.conv.weight: 62
model.decoder.layers.9.linear2.weight: 4096
model.decoder.layers.9.linear2.bias: 64
model.decoder.layers.9.conv_layer_norm.weight: 64
model.decoder.layers.9.conv_layer_norm.bias: 64
model.decoder.layers.9.fc1.weight: 16384
model.decoder.layers.9.fc1.bias: 256
model.decoder.layers.9.fc2.weight: 16384
model.decoder.layers.9.fc2.bias: 64
  0%|                                                                                                                                  | 0/100001 [00:00<?, ?it/s]
  0%|                                                                                                                                    | 0/1000 [00:00<?, ?it/s]
model.decoder.layers.9.final_layer_norm.weight: 64
model.decoder.layers.9.final_layer_norm.bias: 64
model.decoder.layers.10.linear1.weight: 8192
model.decoder.layers.10.linear1.bias: 128
model.decoder.layers.10.conv.weight: 62
model.decoder.layers.10.linear2.weight: 4096
model.decoder.layers.10.linear2.bias: 64
model.decoder.layers.10.conv_layer_norm.weight: 64
model.decoder.layers.10.conv_layer_norm.bias: 64
model.decoder.layers.10.fc1.weight: 16384
model.decoder.layers.10.fc1.bias: 256
model.decoder.layers.10.fc2.weight: 16384
model.decoder.layers.10.fc2.bias: 64
model.decoder.layers.10.final_layer_norm.weight: 64
model.decoder.layers.10.final_layer_norm.bias: 64
model.decoder.layers.11.linear1.weight: 8192
model.decoder.layers.11.linear1.bias: 128
model.decoder.layers.11.conv.weight: 62
model.decoder.layers.11.linear2.weight: 4096
model.decoder.layers.11.linear2.bias: 64
model.decoder.layers.11.conv_layer_norm.weight: 64
model.decoder.layers.11.conv_layer_norm.bias: 64
model.decoder.layers.11.fc1.weight: 16384
model.decoder.layers.11.fc1.bias: 256
model.decoder.layers.11.fc2.weight: 16384
model.decoder.layers.11.fc2.bias: 64
model.decoder.layers.11.final_layer_norm.weight: 64
model.decoder.layers.11.final_layer_norm.bias: 64
n_params=550496
ignored=[('embedder.embedder.weight', 320), ('embedder.embedder.bias', 64), ('head.head.weight', 64), ('model.decoder.decoder_embedder.weight', 64)]
[[split=train]] train_iter: 0, batch_idx: 0, input: [[0.8162900805473328, -0.8937985897064209, -1.231592059135437, -0.006909837480634451, -0.6606903672218323], [0.8797289133071899, 0.0, 0.0, 0.0, 0.0], [1.338809609413147, 1.693846344947815, 0.37473538517951965, -2.216855049133301, -1.0760955810546875], [-2.262162923812866, 0.0, 0.0, 0.0, 0.0], [0.4095987379550934, -0.959525465965271, -0.40346845984458923, 0.10880132764577866, -0.7321749329566956], [1.30717134475708, 0.0, 0.0, 0.0, 0.0], [-0.6105225086212158, -1.8265992403030396, 0.3599432110786438, 0.5522516369819641, 0.9502310156822205], [0.5753015279769897, 0.0, 0.0, 0.0, 0.0], [-1.1912099123001099, 0.7222743630409241, -0.675369381904602, -0.3983597755432129, 0.9675205945968628], [-0.4647520184516907, 0.0, 0.0, 0.0, 0.0]], target: [0.8797289133071899, -2.262162923812866, 1.30717134475708, 0.5753015279769897, -0.4647520184516907, -0.9855940937995911, 1.3876891136169434, 0.9503867626190186, -1.9472428560256958, 0.09046852588653564]
[[split=train]] train_iter: 0, batch_idx: 1, input: [[-0.4491397440433502, 1.8280136585235596, 0.9897819757461548, -0.320390909910202, 0.5379754900932312], [-3.1020119190216064, 0.0, 0.0, 0.0, 0.0], [0.0064454516395926476, 0.15073370933532715, 0.17710530757904053, 0.04110562428832054, 1.1405218839645386], [-0.8037629723548889, 0.0, 0.0, 0.0, 0.0], [0.3398081958293915, 0.08187104016542435, 0.665878176689148, 1.0534658432006836, -0.07220642268657684], [-0.6552398204803467, 0.0, 0.0, 0.0, 0.0], [-1.1079179048538208, 0.2800193727016449, 0.14736108481884003, 0.5868382453918457, -1.0011214017868042], [-0.34071919322013855, 0.0, 0.0, 0.0, 0.0], [-0.547766387462616, -1.1050153970718384, 0.925240159034729, -1.4807944297790527, 0.6986226439476013], [-1.229428768157959, 0.0, 0.0, 0.0, 0.0]], target: [-3.1020119190216064, -0.8037629723548889, -0.6552398204803467, -0.34071919322013855, -1.229428768157959, 3.2118775844573975, -2.7036781311035156, 0.0024459362030029297, -2.583981513977051, -1.1445428133010864]
[[split=eval]] train_iter: 0, eval_iter: 0, batch_idx: 0, input: [[0.056923117488622665, -0.2254948914051056, 0.3654947280883789, -0.12143641710281372, -0.26607635617256165], [0.5323647260665894, 0.0, 0.0, 0.0, 0.0], [0.6566091775894165, -0.349270224571228, 0.934318482875824, 0.32319915294647217, 0.03453914076089859], [1.0814130306243896, 0.0, 0.0, 0.0, 0.0], [1.1539881229400635, -0.2999887466430664, 0.11472588777542114, -0.23394045233726501, -0.41918471455574036], [2.7665770053863525, 0.0, 0.0, 0.0, 0.0], [-1.296101450920105, 0.9774020910263062, -1.100465178489685, -0.16676835715770721, 0.44746509194374084], [-2.677063226699829, 0.0, 0.0, 0.0, 0.0], [1.8922678232192993, -0.30407023429870605, 0.45499151945114136, 1.6132011413574219, -1.9200893640518188], [7.698993682861328, 0.0, 0.0, 0.0, 0.0]], target: [0.5323647260665894, 1.0814130306243896, 2.7665770053863525, -2.677063226699829, 7.698993682861328, 1.873331904411316, 0.7060158252716064, -0.30461013317108154, -0.6633553504943848, 0.9465861916542053]
[[split=eval]] train_iter: 0, eval_iter: 1, batch_idx: 0, input: [[0.32853397727012634, -1.113255262374878, 0.838888943195343, -0.6574654579162598, -0.3239317536354065], [1.1324061155319214, 0.0, 0.0, 0.0, 0.0], [1.142399787902832, -0.038401633501052856, 1.4896352291107178, 0.07022134214639664, -1.397045373916626], [-1.6001558303833008, 0.0, 0.0, 0.0, 0.0], [1.96773362159729, 0.31781065464019775, -1.793107509613037, 0.3199353814125061, -0.9854117631912231], [0.5978349447250366, 0.0, 0.0, 0.0, 0.0], [-1.1753662824630737, -0.7069042921066284, 1.631754994392395, 0.33515477180480957, 0.7220890522003174], [-0.8600477576255798, 0.0, 0.0, 0.0, 0.0], [-0.14661362767219543, -1.3920907974243164, -0.6477355360984802, -0.11829765141010284, -1.3703991174697876], [-0.6125239133834839, 0.0, 0.0, 0.0, 0.0]], target: [1.1324061155319214, -1.6001558303833008, 0.5978349447250366, -0.8600477576255798, -0.6125239133834839, 5.261972427368164, 3.258908748626709, 0.8352506160736084, 1.506016492843628, 2.41762113571167]



  4%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                                      | 35/1000 [00:11<02:20,  6.87it/s]