- data=omniglot
- data.seq_config.p_bursty=1
- eval.every=5000
- eval.iters=1000
- model=mamba
- model.d_model=64
- model.max_seq_len=64
- model.n_layer=12
- optimizer=adamw
- optimizer.betas=[0.9, 0.95]
- optimizer.lr=0.000316228
- optimizer.weight_decay=0
- scheduler.decay_lr=True
- seed=2059
- train.batch_size=128
- train.do_early_stop=False
- train.iters=100001
- train.log_every=1000
- train.num_workers=4
- train.parallel_loss=False
