- data=lang-model
- data.preprocessing_num_workers=20
- data.version=original
- eval.every=5000
- eval.iters=1000
- model=mamba
- model.d_model=800
- model.max_seq_len=512
- model.n_layer=8
- optimizer=adamw
- optimizer.lr=0.000316228
- optimizer.weight_decay=0
- save_checkpoints=True
- scheduler.decay_lr=True
- seed=2059
- train.batch_size=50
- train.do_early_stop=False
- train.iters=200001
- train.log_every=1000
- train.num_workers=0
- train.parallel_loss=True
- train.save_every=50000
