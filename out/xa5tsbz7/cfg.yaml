seed: 2059
seed_eval: 2000
seed_test: 3000
device: cuda
train:
  do: true
  do_save: false
  reset_every: null
  dtype: float32
  compile: false
  do_amp: false
  batch_size: 128
  grad_clip: 1.0
  iters: 100001
  samples: null
  log_every: 1000
  save_every: 100000
  save_path: ./out
  num_workers: 0
  do_early_stop: false
  early_stop_patience: 5
  early_stop_metric: loss
  early_stop_tol: 0.001
  early_stop_start_iter: ${scheduler.warmup_iters}
  early_stop_acc: null
  parallel_loss: true
  merge_embeds: false
  merge_type: sum
eval:
  do: true
  split: both
  batch_size: 1
  every: 5000
  every_samples: null
  iters: 1000
scheduler:
  decay_lr: true
  warmup_iters: ${eval:'${train.iters} * 0.2'}
  lr_decay_iters: ${train.iters}
  min_lr: ${eval:'${optimizer.lr} / 10'}
optimizer:
  lr: 0.001
  _name_: adamw
  weight_decay: 0
  betas:
  - 0.9
  - 0.95
log_level: info
examples_to_log: 3
log_batch_idx:
- 0
- 1
wandb:
  project: icl-arch
save_checkpoints: false
nl_icl:
  do: false
  checkpoint_path: null
  hf_path: null
  task: sentiment
  n_seeds: 10
  min_examples_per_class: 0
  max_examples_per_class: 9
  do_full_vocab: true
do_count_param_only: false
model:
  _name_: mamba
  d_model: 64
  n_layer: 12
  norm_epsilon: 1.0e-05
  rms_norm: false
  fused_add_norm: false
  residual_in_fp32: false
  max_seq_len: 4096
data:
  _name_: linear-regression
  curriculum:
    dims:
      start: 5
      end: 5
      inc: 1
      interval: 2000
    points_train:
      start: 32
      end: 32
      inc: 2
      interval: 2000
    points_val:
      start: 1024
      end: 1024
      inc: 2
      interval: 2000
  task: linear_regression
  data: gaussian
  task_kwargs: {}
  n_dims: ${data.curriculum.dims.end}
  train_noise: 0
  val_noise: 0
run_path: out/xa5tsbz7
